{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4a9225-93ee-48a9-9c50-c804db41ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimplifiedCrossCondTransBase(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vq=1024, embed_dim=512, block_size=16):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(num_vq + 2, embed_dim)  # Token Embedding\n",
    "        self.cond_emb = nn.Linear(5, embed_dim)  # Clip Feature Embedding\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim)  # Positional Embedding\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, clip_feature):\n",
    "        # Check for empty idx\n",
    "        #print(f'idx: {idx}')\n",
    "        if len(idx) == 0:\n",
    "            token_embeddings = self.cond_emb(clip_feature).unsqueeze(1)\n",
    "        else:\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "            \n",
    "            # Get token embeddings\n",
    "            token_embeddings = self.tok_emb(idx)\n",
    "            \n",
    "            # Concatenate the clip_feature embeddings at the beginning\n",
    "            token_embeddings = torch.cat([self.cond_emb(clip_feature).unsqueeze(1), token_embeddings], dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(0, token_embeddings.size(1)).unsqueeze(0).to(idx.device)\n",
    "        pos_embeddings = self.pos_embed(positions)\n",
    "        x = token_embeddings + pos_embeddings\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e71d082-47be-4be9-bbbc-1419f4f6f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SimplifiedCrossCondTransBase()\n",
    "\n",
    "# Mock data\n",
    "idx = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Mock token indices for 2 samples in batch\n",
    "clip_feature = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1]])  # Mock clip features for 2 samples in batch\n",
    "\n",
    "# Forward pass\n",
    "output = model(idx, clip_feature)\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([2, 4, 512]) since for each sample, we now have 4 tokens (1 for clip feature + 3 original tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cd8399-fa4a-4023-bc39-e2f53355ec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = idx.size()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8610c5-0149-4424-b8e4-6b928b5a81c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape (before concat): torch.Size([2, 3, 512])\n",
      "Token embeddings shape (after concat): torch.Size([2, 4, 512])\n",
      "Positional embeddings shape: torch.Size([1, 4, 512])\n",
      "Final output shape: torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimplifiedCrossCondTransBase(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vq=1024, embed_dim=512, block_size=16):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(num_vq + 2, embed_dim)  # Token Embedding, +2 for padding\n",
    "        self.cond_emb = nn.Linear(5, embed_dim)  # Clip Feature Embedding ## output shape\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim)  # Positional Embedding\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, clip_feature):\n",
    "        # Check for empty idx\n",
    "        if len(idx) == 0:\n",
    "            token_embeddings = self.cond_emb(clip_feature).unsqueeze(1)\n",
    "            print(\"Token embeddings shape (when idx is empty):\", token_embeddings.shape)\n",
    "        else:\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "            \n",
    "            # Get token embeddings\n",
    "            token_embeddings = self.tok_emb(idx)\n",
    "            print(\"Token embeddings shape (before concat):\", token_embeddings.shape)\n",
    "            \n",
    "            # Concatenate the clip_feature embeddings at the beginning\n",
    "            token_embeddings = torch.cat([self.cond_emb(clip_feature).unsqueeze(1), token_embeddings], dim=1)\n",
    "            print(\"Token embeddings shape (after concat):\", token_embeddings.shape)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(0, token_embeddings.size(1)).unsqueeze(0).to(idx.device)\n",
    "        pos_embeddings = self.pos_embed(positions)\n",
    "        print(\"Positional embeddings shape:\", pos_embeddings.shape)\n",
    "        \n",
    "        x = token_embeddings + pos_embeddings\n",
    "        print(\"Final output shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimplifiedCrossCondTransBase()\n",
    "\n",
    "# Mock data\n",
    "idx = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Mock token indices for 2 samples in batch\n",
    "clip_feature = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1]])  # Mock clip features for 2 samples in batch\n",
    "\n",
    "# Forward pass\n",
    "output = model(idx, clip_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1c2e47-1f48-41e7-a5cc-8d3641e36cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of token_embeddings: torch.Size([2, 4, 3])\n",
      "Shape of clip_feature_embedding: torch.Size([2, 1, 3])\n",
      "Shape after concatenation: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 2\n",
    "embed_dim = 3\n",
    "clip_feature = torch.randn(batch_size, 5)  # [2, 5]\n",
    "token_embeddings = torch.randn(batch_size, 4, embed_dim)  # [2, 4, 3]\n",
    "\n",
    "# Linear layer to simulate self.cond_emb\n",
    "cond_emb = nn.Linear(5, embed_dim)\n",
    "\n",
    "# Embed the clip_feature\n",
    "clip_feature_embedding = cond_emb(clip_feature).unsqueeze(1)  # [2, 1, 3]\n",
    "\n",
    "# Concatenate\n",
    "concatenated = torch.cat([clip_feature_embedding, token_embeddings], dim=1)  # [2, 5, 3]\n",
    "\n",
    "print(\"Shape of token_embeddings:\", token_embeddings.shape)\n",
    "print(\"Shape of clip_feature_embedding:\", clip_feature_embedding.shape)\n",
    "print(\"Shape after concatenation:\", concatenated.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c4e5c8-3048-4fe7-8542-a2b0fec99131",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_kit = torch.load('../pretrained/VQVAE_KIT/net_best_fid.pth', map_location ='cpu')\n",
    "codebook = ckpt_kit['net']['vqvae.quantizer.codebook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a761da-65c5-4a51-81ec-b26805a0d010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac354e-5f44-457f-a084-86be752f274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd65816-724c-4b7a-ad94-29aec7385304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.9491, -0.8419,  0.5876,  0.5137, -0.7643], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.weights\n",
    "\n",
    "model = SimpleModel(5)\n",
    "for param in model.parameters():\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860c3b9-ac9e-433c-87b8-ea1d7c59b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming codebook is loaded and its shape is [512, 512]\n",
    "#codebook = torch.randn(512, 512)\n",
    "\n",
    "# Generate a random idx tensor of shape [batch_size, sequence_length]\n",
    "# where each value is an index to the codebook.\n",
    "batch_size = 10\n",
    "sequence_length = 20\n",
    "idx = torch.randint(0, 512, (batch_size, sequence_length))  # random indices between 0 and 511\n",
    "print(f'idx: {idx}')\n",
    "# Fetch the embeddings for each idx\n",
    "token_embeddings = torch.index_select(codebook, 0, idx.view(-1)).view(idx.size(0), idx.size(1), -1)\n",
    "\n",
    "# Print the shape of token_embeddings\n",
    "print(token_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46480228-ae20-467f-96f8-424ca02c81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7125d59-866e-4bb5-a510-581be518a289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[279, 267, 101, 237, 422, 213, 192,  58, 160, 391, 117, 198,  18,   2,\n",
       "         457, 478, 453, 139, 502, 160],\n",
       "        [502, 243,  91, 491, 446, 388, 250, 195, 241, 247, 436, 124, 132, 359,\n",
       "          50, 189, 347, 248, 395, 422],\n",
       "        [170, 446, 201, 234, 276, 419, 225,  65, 362, 365, 380, 458, 279, 130,\n",
       "         287, 497, 406,   1, 448, 463],\n",
       "        [262, 381, 258, 491,  38, 135, 209, 134, 358, 171, 271,  21,  70, 491,\n",
       "         344, 504,  28,  58, 164,  36],\n",
       "        [ 66, 459, 297,  37, 188,  24, 494, 322, 242, 165, 401, 440, 252, 192,\n",
       "          39, 245, 326, 439, 187, 468],\n",
       "        [200, 107, 225, 195, 255, 347, 320, 501, 260, 369, 310, 329,  30, 166,\n",
       "         250,  92, 457, 139, 268,  28],\n",
       "        [175, 474, 266,  37, 272, 471, 160, 360,  45, 326, 324, 241, 167,  13,\n",
       "         311, 282,  66, 396, 169,   8],\n",
       "        [ 54,  60,  61, 136, 151, 371, 243, 379, 124, 243, 124, 499, 456,  95,\n",
       "          99, 304,  46, 103, 445, 134],\n",
       "        [282, 317, 148, 235, 151,  56, 458, 506, 117, 429, 322, 183, 220, 359,\n",
       "         375, 266,  33, 265, 111, 270],\n",
       "        [ 75,  22, 198, 135, 267, 490, 143, 443, 129,  52, 282, 408,  23, 268,\n",
       "          25, 124, 440, 360, 292, 368]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efba414-4978-46cc-9939-cccca10e6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d5ed6f-b033-4824-aae2-035b7c787685",
   "metadata": {},
   "outputs": [],
   "source": [
    "em1 = torch.index_select(codebook, 0, idx.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "425c600e-51ec-4a03-9ac1-434cbc617989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b0d55d-52ff-406a-9d1c-0d2e8d9fe944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8887, 0.0367],\n",
       "        [0.9229, 0.0377],\n",
       "        [0.7410, 0.6245]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(2, 3)\n",
    "reshaped = tensor.view(3, 2)  # Reshape it to [3, 2]\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1c6de07-6503-45e6-bb21-99520e762077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8400, 0.3243, 0.2485],\n",
      "        [0.2472, 0.8400, 0.4535]])\n",
      "tensor([[0.8400, 0.3243],\n",
      "        [0.2485, 0.2472],\n",
      "        [0.8400, 0.4535]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2, 3)\n",
    "print(tensor)\n",
    "reshaped = tensor.reshape(3, 2)  # Reshape it to [3, 2]\n",
    "print(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a1db383-5411-4ba0-82d0-4d395f714e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8169, 0.9610, 0.7981, 0.0943, 0.6440, 0.8018])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(2, 3)\n",
    "reshaped = tensor.reshape(-1)  # Reshape it to [3, 2]\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e679ae-c3cb-4bd1-a9ff-9489eaaaf48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2M",
   "language": "python",
   "name": "t2m-gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
